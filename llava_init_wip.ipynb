{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, CLIPImageProcessor, CLIPVisionModel, AutoConfig\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from vicuna_llava import vicuna_llava, dataset_llava\n",
    "from torch.utils.data import DataLoader\n",
    "import zipfile\n",
    "import wget\n",
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a586e1733e0246019dc3d89cf5d818ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\colli\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "C:\\Users\\colli\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "C:\\Users\\colli\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\colli\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# grab vicuna and its tokenizer\n",
    "model_name = \"lmsys/vicuna-7b-v1.5\"\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "vicunallava = accelerator.prepare(vicuna_llava(config))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directories pointing to images directory and chat.json, I've downloaded the dataset locally to deal with some of the missing images\n",
    "chat = 'CC3M/chat.json'\n",
    "im_dir = \"CC3M/images/\"\n",
    "\n",
    "cc3m_dataset = dataset_llava(chat, im_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't compute gradient on any vicunallava layers besides im_embedding\n",
    "for i in iter(vicunallava.parameters()):\n",
    "    i.requires_grad = False\n",
    "\n",
    "for i in vicunallava.im_embedding.parameters():\n",
    "    i.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(testprompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "testprompt, testimage, _ = cc3m_dataset[15]\n",
    "\n",
    "output = vicunallava.generate(testimage, testprompt, max_new_tokens=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training (Stage 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer =  accelerator.prepare(AdamW(vicunallava.parameters(), lr=2e-3))\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='none', ignore_index=-100)\n",
    "batch_size=1#128\n",
    "losses = []\n",
    "\n",
    "cc3m_dataloader =  accelerator.prepare(DataLoader(cc3m_dataset, batch_size=batch_size))\n",
    "\n",
    "vicunallava.train()\n",
    "for i in range(1):\n",
    "    batchiter = 0\n",
    "\n",
    "    for batchprompt,batchimage,batchresp in cc3m_dataloader:\n",
    "        \n",
    "\n",
    "        input = [batchprompt[i]+'###'+batchresp[i] for i in range(batch_size)]\n",
    "\n",
    "        tokenized_input = vicunallava.tokenize(input)\n",
    "\n",
    "        encoded_batchim = vicunallava.vision_tower(batchimage)\n",
    "\n",
    "        separator_token_id = vicunallava.tokenizer.convert_tokens_to_ids(\"###\")\n",
    "\n",
    "        # call model on encoded/tokenized inputs\n",
    "        outs = vicunallava(batchimage, input, batch_size=batch_size)\n",
    "\n",
    "        # shift outputs for causal loss calculation\n",
    "        shifted_outs = outs[:,:-1,:]\n",
    "        shifted_labels = tokenized_input['input_ids'][:,1:]\n",
    "\n",
    "        # mask image out of loss computation\n",
    "        im_loss_mask = torch.full_like(encoded_batchim[:,:,0], -100)\n",
    "\n",
    "        labels = shifted_labels.clone()\n",
    "        labels[(labels == separator_token_id).cumsum(dim=1) == 0] = -100\n",
    "\n",
    "        # loss mask will ensure loss is only computed over the response tokens\n",
    "        lossmask = torch.cat((im_loss_mask, labels), dim=1).type(torch.LongTensor)\n",
    "\n",
    "        # compute loss, append it to list\n",
    "        loss = loss_fn(shifted_outs.reshape(-1,vicunallava.model.config.vocab_size), lossmask.reshape(-1))\n",
    "        losses.append(loss)\n",
    "\n",
    "        # compute loss per sample only over non-masked elements\n",
    "        loss_per_sample = loss[loss != 0].mean()\n",
    "\n",
    "        # optimize model\n",
    "        accelerator.backward(loss_per_sample)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        batchiter+=1\n",
    "        print(f\"iteration:{batchiter}, loss:{loss_per_sample}\")\n",
    "        torch.save(vicunallava.im_embedding, 'vicunallava_im_embedding_stage1.pt')\n",
    "\n",
    "\n",
    "        # takes ~10min for me to locally train through 5 batches of size 20"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
