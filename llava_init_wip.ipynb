{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, CLIPImageProcessor, CLIPVisionModel, AutoConfig\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.transforms as T\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from vicuna_llava import vicuna_llava, dataset_llava\n",
    "\n",
    "import zipfile\n",
    "import wget\n",
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log in to huggingface (using llama requires you to request access on huggingface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18ffbd47f61247728617f97a072350a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate vicunallava"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "\n",
    "# grab vicuna and its tokenizer\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"#\"lmsys/vicuna-7b-v1.5\"\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "vicunallava = accelerator.prepare(vicuna_llava(config, llmURL=model_name, accelerator=accelerator))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab pretrained projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear_llava_proj = torch.load('mm_projector.bin', weights_only=True)\n",
    "#linear_llava_weights = linear_llava_proj['model.mm_projector.weight']\n",
    "#linear_llava_biases = linear_llava_proj['model.mm_projector.bias']\n",
    "\n",
    "#with torch.no_grad():\n",
    "#    vicunallava.im_embedding.weight.copy_(linear_llava_weights)\n",
    "#    vicunallava.im_embedding.bias.copy_(linear_llava_biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab dataset or subset of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directories pointing to images directory and chat.json, I've downloaded the dataset locally to deal with some of the missing images\n",
    "chat = 'CC3M/chat.json'\n",
    "im_dir = \"CC3M/images/\"\n",
    "\n",
    "subset_idcs = list(range(100))\n",
    "\n",
    "cc3m_dataset = accelerator.prepare(Subset(dataset_llava(chat, im_dir), subset_idcs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set which parameters to be optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't compute gradient on any vicunallava layers besides im_embedding\n",
    "for i in iter(vicunallava.parameters()):\n",
    "    i.requires_grad = False\n",
    "for i in iter(vicunallava.model.parameters()):\n",
    "    i.requires_grad = False\n",
    "for i in vicunallava.im_embedding.parameters():\n",
    "    i.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Generating on Some Sample from Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#testprompt, testimage, _ = cc3m_dataset[26]\n",
    "#testimage = testimage.to(device)\n",
    "#transform = T.ToPILImage()\n",
    "\n",
    "#output = vicunallava.generate(testimage, testprompt, max_new_tokens=20)\n",
    "#print(f'input prompt: {testprompt}/n')\n",
    "#transform(testimage).show()\n",
    "\n",
    "#print(f'response: {output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training (Stage 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\colli\\AppData\\Local\\Temp\\ipykernel_22144\\835370053.py:24: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:1, loss:11.599148750305176\n",
      "iteration:2, loss:11.623420715332031\n",
      "iteration:3, loss:11.605378150939941\n",
      "iteration:4, loss:11.62193489074707\n",
      "iteration:5, loss:11.617191314697266\n",
      "iteration:6, loss:11.574458122253418\n",
      "iteration:7, loss:11.648216247558594\n",
      "iteration:8, loss:11.632831573486328\n",
      "iteration:9, loss:11.567292213439941\n",
      "iteration:10, loss:11.611964225769043\n",
      "iteration:1, loss:11.590994834899902\n",
      "iteration:2, loss:11.624755859375\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer =  accelerator.prepare(AdamW(vicunallava.parameters(), lr=2e-3))\n",
    "loss_fn = accelerator.prepare(nn.CrossEntropyLoss(reduction='none', ignore_index=-100))\n",
    "batch_size=10\n",
    "losses = []\n",
    "\n",
    "cc3m_dataloader =  accelerator.prepare(DataLoader(cc3m_dataset, batch_size=batch_size))\n",
    "\n",
    "\n",
    "vicunallava.train()\n",
    "for i in range(10000):\n",
    "    batchiter = 0\n",
    "\n",
    "    for batchprompt,batchimage,batchresp in cc3m_dataloader:\n",
    "\n",
    "        input = [batchprompt[i]+'###'+batchresp[i] for i in range(batch_size)]\n",
    "\n",
    "        tokenized_input = vicunallava.tokenize(input)\n",
    "\n",
    "        encoded_batchim = vicunallava.vision_tower(batchimage)\n",
    "\n",
    "        separator_token_id = vicunallava.tokenizer.convert_tokens_to_ids(\"###\")\n",
    "        with autocast():\n",
    "            # call model on encoded/tokenized inputs\n",
    "            outs = vicunallava(batchimage, input, batch_size=batch_size)\n",
    "            # shift outputs for causal loss calculation\n",
    "            shifted_outs = outs[:,:-1,:]\n",
    "            shifted_labels = tokenized_input['input_ids'][:,1:]\n",
    "            # mask image out of loss computation\n",
    "            im_loss_mask = torch.full_like(encoded_batchim[:,:,0], -100)\n",
    "            labels = shifted_labels.clone()\n",
    "            labels[(labels == separator_token_id).cumsum(dim=1) == 0] = -100\n",
    "            # loss mask will ensure loss is only computed over the response tokens\n",
    "            lossmask = torch.cat((im_loss_mask, labels), dim=1).type(torch.LongTensor).to(device)\n",
    "            # compute loss, append it to list\n",
    "            loss = loss_fn(shifted_outs.reshape(-1,vicunallava.model.config.vocab_size), lossmask.reshape(-1)).to(device)\n",
    "            losses.append(loss)\n",
    "\n",
    "        # compute loss per sample only over non-masked elements\n",
    "        loss_per_sample = loss[loss != 0].mean()\n",
    "\n",
    "        # optimize model\n",
    "        accelerator.backward(loss_per_sample)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        batchiter+=1\n",
    "        print(f\"iteration:{batchiter}, loss:{loss_per_sample}\")\n",
    "    if i%10 == 0:\n",
    "        torch.save(vicunallava.im_embedding, 'vicunallava_im_embedding_stage1.pt')\n",
    "\n",
    "\n",
    "            # takes ~10min for me to locally train through 5 batches of size 20"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
