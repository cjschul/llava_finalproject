{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from vicuna_llava import vicuna_llava, dataset_llava\n",
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log in to huggingface (using llama requires you to request access on huggingface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5512014405084f69ae7f07ec70736fd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate vicunallava"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# grab vicuna and its tokenizer\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"#\"lmsys/vicuna-7b-v1.5\"\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "vicunallava = accelerator.prepare(vicuna_llava(config, llmURL=model_name, accelerator=accelerator))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab pretrained projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear_llava_proj = torch.load('mm_projector.bin', weights_only=True)\n",
    "#linear_llava_weights = linear_llava_proj['model.mm_projector.weight']\n",
    "#linear_llava_biases = linear_llava_proj['model.mm_projector.bias']\n",
    "\n",
    "#with torch.no_grad():\n",
    "#    vicunallava.im_embedding.weight.copy_(linear_llava_weights)\n",
    "#    vicunallava.im_embedding.bias.copy_(linear_llava_biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directories pointing to images directory and chat.json, I've downloaded the dataset locally to deal with some of the missing images\n",
    "chat = 'CC3M/chat.json'\n",
    "im_dir = \"CC3M/images/\"\n",
    "\n",
    "num_samples=12\n",
    "subset_idcs = list(range(num_samples))\n",
    "\n",
    "cc3m_dataset = accelerator.prepare(Subset(dataset_llava(chat, im_dir), subset_idcs))\n",
    "batch_size=3\n",
    "cc3m_dataloader =  accelerator.prepare(DataLoader(cc3m_dataset, batch_size=batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Generating on Some Sample from Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testprompt, testimage, _ = cc3m_dataset[0]\n",
    "#testimage = testimage.to(device)\n",
    "#transform = T.ToPILImage()\n",
    "\n",
    "#output = vicunallava.generate(testimage, testprompt, max_new_tokens=15)\n",
    "#print(f'input prompt: {testprompt}')\n",
    "#transform(testimage).show()\n",
    "\n",
    "#print(f'response: {output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training (Stage 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set which parameters to be optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't compute gradient on any vicunallava layers besides im_embedding\n",
    "for i in iter(vicunallava.parameters()):\n",
    "    i.requires_grad = False\n",
    "for i in iter(vicunallava.model.parameters()):\n",
    "    i.requires_grad = False\n",
    "for i in iter(vicunallava.im_embedding.parameters()):\n",
    "    i.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\colli\\OneDrive\\Documents\\Classes\\UofM\\ECE 598 (LLMs)\\Final Project\\llava_finalproject\\llava_finalproject\\vicuna_llava.py:100: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, loss:11.723671913146973\n",
      "epoch:1, loss:11.667545318603516\n",
      "epoch:2, loss:11.67780590057373\n",
      "epoch:3, loss:11.65052604675293\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "from transformers import get_scheduler\n",
    "\n",
    "\n",
    "optimizer =  accelerator.prepare(Adam([{\"params\":vicunallava.parameters(),\n",
    "                                        \"params\":vicunallava.im_embedding.parameters()}], lr=2e-3))\n",
    "\n",
    "num_epochs = 1000\n",
    "num_dataset_samples = len(cc3m_dataset)\n",
    "num_batch_steps = num_epochs*num_dataset_samples/batch_size\n",
    "num_warmup_steps = np.ceil(0.03*num_batch_steps)\n",
    "lr_scheduler = get_scheduler(\"cosine\",\n",
    "                             optimizer=optimizer,\n",
    "                             num_warmup_steps=num_warmup_steps,\n",
    "                             num_training_steps=num_batch_steps)\n",
    "\n",
    "losses = []\n",
    "\n",
    "vicunallava.train()\n",
    "for i in range(num_epochs):\n",
    "    batchiter = 0\n",
    "\n",
    "    for batchprompt,batchimage,batchresp in cc3m_dataloader:\n",
    "        \n",
    "        input = [batchprompt[j]+'###'+batchresp[j] for j in range(batch_size)]\n",
    "        input = vicunallava.process_prompt(input)\n",
    "        tokenized_input = vicunallava.tokenize(input)\n",
    "        outs = vicunallava(batchimage, input, batch_size=batch_size)\n",
    "        losses.append(outs['loss'])\n",
    "\n",
    "        # optimize model\n",
    "        accelerator.backward(outs['loss'])\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        batchiter+=1\n",
    "    print(f\"epoch:{i}, loss:{outs['loss']}\")    \n",
    "    if i%10 == 0:\n",
    "        torch.save(vicunallava.im_embedding, 'vicunallava_im_embedding_stage1.pt')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
